

\begin{comment}

\subsection{MNIST}
To begin our exploration of GANs, we decided to build off of the work done by Radford et al. in their creation of the DCGAN model. Due to the generally strong performance of the DCGAN model across a variety of generative tasks for images, we decided that it would be a good starting point. We started by using the DCGAN model on the MNIST dataset of 60,000 images, without labels, for a vanilla GAN task. Since DCGAN is such a powerful and deep GAN model, we found that it trained very easily on the MNIST dataset. Even after only 10 epochs, we found the generated output to be quite reasonable and expected that by running the model for longer we could have high quality generated images for hand-written digits. A sample of the generated output after only 10 epochs is shown below:

\subsection{CIFAR-10}
In order to take on a more challenging task, we decided try training GANs on the CIFAR-10 dataset\footnote{\url{https://www.cs.toronto.edu/~kriz/cifar.html}}. The CIFAR-10 dataset consists of a set of images with 10 classes: \{airplane, automobile, bird, cat, deer, dog, frog, horse, ship, truck\}, and the images are "tiny" so we found that the resolution quality of the images is not very good. We started by training the original DCGAN model on this dataset. Generated output of the model after 40 epochs is shown below:

We wanted to incorporate the conditional GAN framework outlined in Conditional Generative Adversarial Nets\footnote{\url{https://arxiv.org/pdf/1411.1784.pdf}} into the DCGAN model, so that we could try utilizing the class information while training on the CIFAR-10 dataset. This would also allow us to do class-conditioned generation of images. As described in the paper, a vanilla GAN can be extended to a conditional GAN by feeding additional information, which could be class labels or data from other modalities, into both the generator and the discriminator. 
\\\\
In the first version of our conditional DCGAN model, we concatenated the input noise vector for the generator and a randomly chosen one-hot encoding of a class label, then passed this entire concatenated vector into the generator. Other than that, the architecture of the generator was exactly as before in the original DCGAN model. The first part of the discriminator worked as before, where an image tensor that came from either a real example in the data or from the generator would be passed through a series of convolution, leaky ReLU, and batch-norm layers. However, after the final convolution layer when we had a single feature vector as the output, this feature vector was concatenated with a one-hot encoding of a class label. If this image being passed through the generator was from the dataset, then its true class label would be used, while if the image was generated by the generator then the class label used for its generation was used. The concatenated vector of the feature vector and the one-hot labels would then be passed through a sigmoid layer, consisting of an affine transformation mapping the input vector to a single output and then a sigmoid non-linearity giving us a probability. 
\\\\
In this model, we now have two new parameters to tune - the dimension of the convolution feature vector in the discriminator that is concatenated with the one-hot label vectors, and the vector representations used to represent class conditionals. First, we experimented with the former, by varying the dimensions of the convolved feature vector between 100, 200, and 400. We found that the dimension of 100 resulted in the lowest quality images, while 200 and 400 were fairly similar, though 400 was slightly better. We concluded that more expressive features from the convolutional network resulted in better performance in the overall model. Below we show output from our conditional model with a convolution feature vector dimension of 400, trained for X epochs:
\\\\
When using the model to generate class-conditioned images, we tried feeding in batches of the same fixed noise vector with each of the class labels to see if the model generated appropriate images for each class. However, we noticed that all of the images with the same fixed noise vector were very similar, even though they had different class labels, as shown below:
\\\\
This motivated our attempt to modify the architecture so that the model put more weight on the class labels in the discriminator and the generator. We thought one issue might be that the magnitude of the values in the feature vector produced by the convolution layers of the discriminator might be much larger than the values of the one-hot encoding vector, which are 9 zeros and 1 one. Our attempt to fix this was to add a batch-normalization layer to this feature vector before it is concatenated with the one-hot encoding vector. However, this did not work well at all and the images produced resembled pure noise. We realized that rather than simply using the one-hot encoded vector, it might improve the model to learn an embedding on this vector. Thus, we added a sigmoid layer to the generator which affine transformed the one-hot encoded vector into a 200 dimensional vector and then applied a sigmoid non-linearity. The rest of the network was the same as before, as this 200 dimensional vector was concatenated with the noise vector and passed through a series of convolutional layers to generate an image. We similarly added a sigmoid layer in the discriminator which again transformed the one-hot encoded vector into a 200 dimension vector, before concatenating it with the convolution feature vector and passing the concatenated vector through the final sigmoid layer to get a probability output. Below, we show some generated images from this version:
\\\\
In comparing the quality of our generated images and the number of epochs run to work done in Conditional Generative Adversarial Nets for Convolutional Face Generation\footnote{\url{http://www.foldl.me/uploads/2015/conditional-gans-face-generation/paper.pdf}}, we noticed that it took approximately 130 epochs of training to obtain high quality generated face images, while between epochs 60 and 80, the generated images had some local qualities of faces such as color patterns and borders but not strong global resemblance to faces. Due to computational resources, we were able to train many of our models in the range of 40 to 60 epochs, and as in the paper, we noticed that our generated images had some local qualities that resembled images from the original data. However, we suspect more training time is needed to get high-quality generated images. Furthermore, the low resolution quality of the original images negatively impacted the quality of our generated images. We did not train our models with GPUs, but we expect that utilizing GPUs would likely have allowed some of our models to train past 100 epochs where we could expect higher quality generated images. 
\end{comment}
